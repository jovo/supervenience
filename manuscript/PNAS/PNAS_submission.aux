\relax 
\citation{Plato97}
\citation{Descartes1641}
\citation{Davidson70}
\citation{SpornsKotter05}
\citation{LichtmanSanes08}
\citation{Seung09}
\citation{Kim95}
\citation{DGL96}
\citation{Devroye83}
\citation{Popper}
\newlabel{def1}{{1}{1}}
\newlabel{def2}{{2}{1}}
\newlabel{thm1}{{1}{1}}
\citation{WhiteBrenner86}
\citation{DenkHorstmann04}
\citation{BriggmanDenk06}
\citation{MackeBorst08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{HornJohnson90}
\citation{ConroyLouis97}
\citation{ZaslavskiyVert08}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\newlabel{proof}{{}{2}}
\newlabel{simulation}{{}{2}}
\citation{VaziriShank08}
\citation{HelmstaedterDenk08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{BuckinghamSattelle08}
\bibcite{Plato97}{1}
\bibcite{Descartes1641}{2}
\bibcite{Davidson70}{3}
\bibcite{SpornsKotter05}{4}
\bibcite{LichtmanSanes08}{5}
\bibcite{Seung09}{6}
\bibcite{Kim95}{7}
\bibcite{DGL96}{8}
\bibcite{Devroye83}{9}
\bibcite{Popper}{10}
\bibcite{WhiteBrenner86}{11}
\bibcite{DenkHorstmann04}{12}
\bibcite{BriggmanDenk06}{13}
\bibcite{MackeBorst08}{14}
\bibcite{Mishchenko09}{15}
\bibcite{LuLichtman09}{16}
\bibcite{HornJohnson90}{17}
\bibcite{ConroyLouis97}{18}
\bibcite{ZaslavskiyVert08}{19}
\bibcite{Durbin87}{20}
\bibcite{deBonoMaricq05}{21}
\bibcite{VarshneyChklovskii09}{22}
\bibcite{ChalasaniBargmann07}{23}
\bibcite{VaziriShank08}{24}
\bibcite{HelmstaedterDenk08}{25}
\bibcite{BuckinghamSattelle08}{26}
\gdef\lastpage{4}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces C.\penalty \@M { }elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n)$ is plotted as a function of class-conditional training sample size $n_j$, suggesting that for $\varepsilon =0.1$ we can determine that $\@mathcal {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_F \@mathcal {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{BM}$. Each dot depicts an estimate for $L_{F}(g_n)$; standard errors are $(L_{F}(g_n)(1-L_{F}(g_n))/1000)^{1/2}$. E.g., $n_j = 180$ ; $k_n = 53$ ; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation.}}{4}}
\newlabel{fig1}{{1}{4}}
