%\input{/Users/joshyv/Research/misc/latex_paper.tex}
\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{times}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\pagestyle{fancy}

\oddsidemargin=0.0in %%this makes the odd side margin go to the default of 1inch
\evensidemargin=0.0in
\textwidth=6.5in
\headwidth=6.5in
\textheight=9in %%sets the textwidth to 6.5, which leaves 1 for the remaining right margin with 8 1/2X11inch paper
\headheight=12pt
\topmargin=-0.25in
%\headheight=0in
%\headsep=0in
%\pagestyle{headings}

\usepackage{hyperref}
% \usepackage{ulem}
% \usepackage{color}

% \newcommand{\loo}{$L^{(1)}_{h; \mD_n}$}
\newcommand{\conv}{\rightarrow}
% \newcommand{\Real}{\mathbb{R}}
% \providecommand{\tr}[1]{\textcolor{red}{#1}}

\newcommand{\mB}{\mathcal{B}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\PP}{\mathbb{P}}           % probability
\newcommand{\EE}{\mathbb{E}}           % expected value
\newcommand{\II}{\mathbb{I}}           % expected value
\newcommand{\Real}{\mathbb{R}}           % expected value

\newcommand{\del}{\delta}
\newcommand{\sig}{\sigma}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\eps}{\varepsilon}

\providecommand{\mc}[1]{\mathcal{#1}}
\providecommand{\mb}[1]{\boldsymbol{#1}}
\providecommand{\mbb}[1]{\mathbb{#1}}
\providecommand{\mv}[1]{\vec{#1}}
\providecommand{\mh}[1]{\widehat{#1}}
\providecommand{\mt}[1]{\widetilde{#1}}
\providecommand{\mhc}[1]{\hat{\mathcal{#1}}}
\providecommand{\mhb}[1]{\hat{\boldsymbol{#1}}}
\providecommand{\mvb}[1]{\vec{\boldsymbol{#1}}}
\providecommand{\mtb}[1]{\widetilde{\boldsymbol{#1}}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}


% \newcommand{\mN}{\mathcal{N}}

\newcommand{\hL}{\widehat{L}}
\newcommand{\MeB}{\mM \overset{\varepsilon}{{\sim}}_{F} \mB}
\newcommand{\MsB}{\mM \overset{S}{\sim}_{F} \mB}
\newcommand{\MnoteB}{\mM \overset{\varepsilon}{{\not\sim}}_{F} \mB}
\providecommand{\tr}[1]{\textcolor{black}{#1}}
\providecommand{\norm}[1]{\left \lVert#1 \right  \rVert}
\newcommand{\T}{^{\ensuremath{\mathsf{T}}}}           % transpose

\newtheorem{defi}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{thex}{\emph{Gedankenexperiment}}
\lhead{Vogelstein JT, et al}
\rhead{Statistical Supervenience}


\begin{document}

	\begin{center}
{\huge	Are mental properties supervenient on brain properties?}
\end{center}

\vspace{5px}

\begin{center}
{\large		
Joshua T. Vogelstein$^{1*}$, R. Jacob Vogelstein$^2$, Carey E. Priebe$^1$\\
	$^1$Department of Applied Mathematics \& Statistics, \\ Johns Hopkins University, Baltimore, MD, 21218,\\ $^2$National Security Technology Department, \\ Johns Hopkins University Applied Physics Laboratory, Laurel, MD 20723}
	
\end{center}

\vspace{5px}

% \maketitle
% \date{\vspace{-5ex}}
%\tableofcontents
% \begin{abstract}
	
\noindent The``mind-brain supervenience'' conjecture suggests that all mental properties are derived from the physical properties of the brain. To address the question of whether the mind supervenes on the brain, we frame a supervenience hypothesis in rigorous statistical terms. Specifically, we propose a modified version of supervenience (called $\eps$-supervenience) that is amenable to experimental investigation and statistical analysis. To illustrate this approach, we perform a thought experiment that illustrates how the probabilistic theory of pattern recognition can be used to make a one-sided determination of $\eps$-supervenience. The physical property of the brain employed in this analysis is the graph describing brain connectivity (i.e., the brain-graph or connectome). $\eps$-supervenience allows us to determine whether a particular mental property can be inferred from one's connectome to within any given misclassification rate > 0, regardless of the relationship between the two. This may provide motivation for cross-disciplinary research between neuroscientists and statisticians.


% \end{abstract}

% \vspace*{0.5 in}

\newpage
% \section*{Introduction}

\noindent The mind-brain supervenience notion was canonized in 1970 with the following quote from Donald Davidson: \cite{Davidson70}
\begin{quotation}
\noindent [mind-brain] supervenience might be taken to mean that there cannot be two events alike in all physical respects but differing in some mental respect, or that an object cannot alter in some mental respect without altering in some physical respect.
\end{quotation}
We consider a special case of supervenience of considerable interest.  Specifically, this work addresses a novel version of a local supervenience between mental properties and brain properties, with particular emphasize on brain-graphs (i.e., connectivity structure).  The determination of supervenience (or lack thereof) of a mental property on a brain property has potentially important implications in a number of fields of inquiry.  Neural network theory and artificial intelligence often implicitly take a generalized notion of local mind-brain supervenience as an assumption; which if falsified, might change modern approaches to learning \cite{Haykin2008,Ripley2008}. Cognitive neuroscience similarly seems to operate under such assumptions, which if falsified, might result in novel perspectives and theories \cite{Fodor1998, Gazzaniga2008}.  And the question of mind-brain supervenience continues to be debated amongst philosophers \cite{Kim2007}.

This work does not attempt to resolve any particular mind-brain supervenience debates.  Rather, we propose a statistical approach for framing mind-brain supervenience questions.  This approach depends on defining the space of mental and brain properties under investigation and a statistical model characterizing the possible distributions governing their relationship.  Such definitions transform supervenience from a conjecture or an assumption, into a hypothesis which can be tested.



\section*{Results}

\subsection*{Statistical supervenience; a definition} % (fold)

\noindent Let $\mc{M}=\{m_1, m_2, \ldots\}$ be a set of possible mental properties. For example, $m$ might indicate a person's intelligence, psychological state, current thought, gender identity, etc.   Similarly, let $\mc{B}=\{b_1,b_2,\ldots\}$ be a set of possible brain properties.  For example, $b$ might denote the number of cells in a person's brain at some time $t$, or the collection of spike trains of all neurons in the brain during some time period $t$ to $t'$. Given these definitions, Davidson's conjecture may be concisely and formally stated thusly:  $m \neq m' \implies b \neq b'$, where $(m,b), (m',b') \in \mc{M} \times \mc{B}$ are mind-brain pairs.  This mind-brain supervenience relation does not imply an injective relation, a causal relation, or an identity relation (see Appendix 1 for more details and some examples).  To facilitate both statistical analysis and empirical investigation, we convert this supervenience relation from a logical to a probabilistic relation.  

% Let $b$ correspond to an agent's brain, which is a particular element from the set of all possible brains, $\mB$. Similarly, let $m$ correspond to an agent's mind, which is a particular element from the set of all possible minds, $\mM$.  
Let $F_{MB}$ indicate a joint distribution of minds and brains. Statistical supervenience can thusly be defined:
\begin{defi}
\label{def1} 
$\mM$ is said to \textit{statistically supervene} on $\mB$ for distribution $F=F_{MB}$, denoted $\mM \overset{S}{\sim}_F \mB$, if and only if $\PP[m \neq m' | b=b']=0$, or equivalently $\PP[m = m' | b = b']=1$. 
\end{defi}
\noindent Statistical supervenience is therefore a probabilistic relation on sets (which could be considered a generalization of correlation; see Appendix 1 for details).  



\subsection*{Statistical supervenience is equivalent to perfect classification accuracy} % (fold)
\label{sub:theoretical_results}

If minds statistically supervene on brains, $\MsB$, then if two minds differ, there must be some brain-based difference to account for the mental difference.  This means that there exists a deterministic function mapping each brain to its supervening mind, $g: \mB \mapsto \mM$. One could therefore, in principle, construct this function. When the space of all possible minds is finite; that is, $|\mM| < \infty$, $g$ is called a \emph{classifier}.  
% Let $\mh{m}$ denote the output of a classifier, $g(b)=\mh{m}$.  
Define misclassification rate, the probability that $g$ misclassifies $b$ under distribution $F=F_{MB}$,  as
\begin{align}
L_{F}(g) = \PP[g(B) \neq M] = \frac{1}{|\mc{M} \times \mc{B}|} \sum_{(m,b) \in \mc{M} \times \mc{B}} \II\{g(b) \neq m\} \PP[B=b, M=m]	
\end{align}
where $\II$ denotes the indicator function taking value unity whenever its argument is true, and zero otherwise.  The Bayes optimal classifier $g^*$ minimizes $L_{F}(g)$ over all classifiers, that is:	
$g^* = \argmin_{g} L_{F}(g)$.
The \emph{Bayes error}, or Bayes risk, $L^*=L_{F}(g^*)$, is the minimum possible misclassification rate.

% The optimal such classifier, $g^*$, has the smallest expected misclassification rate, $L^*=L_{F}(g^*)$, under distribuion $F$.  The minimum misclassification rate is called Bayes error (see Methods for details). 
The primary result of casting supervenience in a statistical framework is the following theorem: 
\begin{thm}
\label{thm1} 
$\mM$ is said to \textit{statistically supervene} on $\mB$ for distribution $F=F_{MB}$, denoted $\mM \overset{S}{\sim}_{F} \mB$, if and only if $L^*= 0$. Formally, \mbox{$\MsB \Leftrightarrow L^*=0$}.  
\end{thm}
\begin{proof}
Let $\mc{M}_b=\{m: \PP[B=b | M=m] >0\}$.  
$$L^*=0 \Leftrightarrow |\mc{M}_b|=1 \, \forall \, b \in \mc{B}.$$
In words, for $L^*$ to equal 0, it must be the case that there exists only a single $m$ for each $b$.
\end{proof}

% \noindent If minds supervene on brains, then, by the definition of supervenience, there exists a function that maps each brain deterministically to a particular mind.  This means that one could draw a decision boundary between all equivalence classes of brains, each class corresponding to a different mind, and no mind will reside within two different equivalence classes.  Thus, the optimal classifier would correctly find these decision boundaries, and therefore have no opportunity to err. $\square$

The above argument shows (for the first time to our knowledge) that statistical supervenience and zero Bayes error are equivalent. Statistical supervenience can therefore be thought of as a constraint on the possible distributions on minds and brains.  Specifically, let $\mc{F}$ indicate the set of all possible joint distributions on minds and brains, and let $\mc{F}_s$ be subset of distributions for which supervenience holds. Theorem \ref{thm1} implies that $\mc{F}_s = \{F_{MB} : L^*=0\} \subseteq \mc{F}$.


\subsection*{The intractability of testing for statistical supervenience} % (fold)
\label{sub:subsection_name}

% subsection subsection_name (end)

Although the above theorem is of potential theoretical interest, the argument relies on knowing the typically unavailable $L^*$, rendering it useless pragmatically.  However,  $g^*$ could be estimated from data, and its estimate could be utilized to conduct the following statistical test of supervenience:
% Let $g_n$ be a classifier induced from some training data, $\mc{T}_n$, 
\begin{align*} 
	H_0:& L^* > 0 \\
	H_A:& L^*=0
\end{align*}
A statistical test proceeds by calculating a test statistic and a critical value $c_\alpha$.  We reject if the test statistic is more extreme than the critical value, or equivalently, if the $p$-value is less than $\alpha$.   The $p$-value, the probability of rejecting a true null hypothesis, is a function of the distribution of the test statistic.  

Let $g_n$ be a classifier induced by the data, $g_n:\mB \times (\mc{M} \times \mc{B})^n \mapsto \mM$.   Assume that training data $\mc{T}_n=\{(M_{1},B_{1}), \ldots, (M_{n},B_{n})\}$ are each sampled identically and independently (iid) from the true (but unknown) joint distribution, $F_{MB}$.  The misclassification rate of a classifier will therefore be a random variable. The expected misclassification rate is given by 
\begin{align}
\EE[L_F(g_n)]=\sum_{(m,b)  \in \mc{M}\times \mc{B}} \II\{g_n(b; \mc{T}_n) \neq m\} \PP[B=b,M=m].
\end{align}
Because calculating the expected misclassification rate requires a sum over all possible mind/brain property pairs, calculating $\EE[L_F(g_n)]$ will often be intractable in practice.  Instead, expected misclassification rate is approximated by ``hold-out'' error.  Let $\mc{H}_{n'}=\{(M_{n+1},B_{n+1}), \ldots, (M_{n+n'},B_{n+n'})\}$ be a set of $n'$ hold-out samples, each sampled iid from $F_{MB}$.  The hold-out approximation to misclassification rate is given by
\begin{align}
\hL^{n'}_{F}(g_{n}) = \sum_{(m,b) \in \mc{H}_{n'}}\II \{g_{n}(b; \mc{T}_{n})\neq m] \PP[B=b,M=m].	
\end{align}
Under the above assumptions, $\hL^{n'}_F(g_n)$ can be used as a surrogate for $L^*$.  By definition of $g^*$, we expect $\hL^{n'}_F(g_n) \geq g^*$ for any $g_n$ and all $n$.   We therefore let $\mh{n}= \lfloor n' \hL^{n'}_{F}(g_n) \rfloor$ be the test statistic, where $\lfloor \cdot \rfloor$ is the floor operator.  The distribution of $\mh{n}$ is available under the least favorable distribution of the alternate hypothesis.  Specifically,  the $p$-value is given by the binomial cumulative distribution function, $\mathbb{B}(\mh{n}; n', p_{H_A})= \sum_{k \in [\mh{n}]}$Binomial$(k; n'; p_{H_A})$, where $p_{H_A}$ is Bernoulli probability under least favorable distribution of the alternate hypothesis and $[\mh{n}]=\{1,\ldots, \mh{n}\}$.  In this simple alternate hypothesis, $p_{H_A}=0$. Because Binomial$(k; n', 0)=1$ for all $n'$, the $p$-value is always equal to unity.    Thus, we can \emph{never} reject the above null hypothesis, no matter how much data we obtain.





\subsection*{$\eps$-supervenience admits hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

We therefore consider a relaxed notion of supervenience:
\begin{defi}
\label{def2}
Given $\varepsilon > 0$, $\mM$ is said to $\varepsilon$-\textit{supervene} on $\mB$ for distribution $F=F_{MB}$, denoted $\MeB$, if and only if $L_{F}(g^*) < \varepsilon$.
\end{defi}
\noindent Given this relaxation, consider the problem of testing for $\eps$-supervenience:
\begin{align*}
	H_0^{\eps}: L^* \geq \eps \\
	H_A^{\eps}: L^* < \eps
\end{align*}
The $p$-value of the above composite alternate hypothesis is given by the binomial distribution under the least favorable distribution, $\mathbb{B}(\mh{n}; n', p_{H_A}=\eps)<1$.  We therefore reject whenever this $p$-value is less than $\alpha$; rejection implies that we are $100(1-\alpha$)\% confident that $\MeB$.   The definition of $\eps$-supervenience therefore admits, for the first time to our knowledge, a viable statistical test of supervenience, given a specified $\eps$ and $\alpha$. 

\subsection*{A \emph{Gedankenexperiment} demonstrating an ideal test} % (fold)
\label{sub:uc}

Ideally, as $n$ increases, the probability of both a Type I error (rejection of a true null hypothesis) and a Type II error (failure to reject a false null hypothesis) goes to zero.  

%For the above described hypothesis of $\eps$-supervenience, both the $p$-value (probability of a type I error)  and power (probability of not making a type II error), will depend on the distribution  of $g_n$.  We therefore consider the properties of $g_n$, and the implications of those properties with regard to the probability of a Type I or Type II error.

The $p$-value 
(probability of a type I error)
of the test statistic for any classifier $g_n$ converges to zero, because $\mathbb{B}(\mh{n}; n', \hL^{n'}_F(g_n))) \conv 0$ as $n,n' \conv \infty$. The power of the test (probability of not making a type II error), however, only converges to unity under certain conditions.  Specifically, let a  \emph{consistent} classifier be any classifier with expected misclassification rate converging to the Bayes optimal limit; that is, $\EE[L_{F}(g_n)] \conv L_{F}(g^*)$ as $n\conv \infty$. The power of the test can be written as: $\PP[\mathbb{B}(\mh{n}; n', \hL^{n'}_F(g_n)) < \mathbb{B}(\mh{n}; n', \eps)]$.  As $n,n' \conv \infty$, $\mathbb{B}(\mh{n}; n', \hL^{n'}_F(g_n)) \conv \mathbb{B}(\mh{n}; n', L^*)$.  And if $L^* < \eps$, then $\mathbb{B}(\mh{n}; n', L^*) < \mathbb{B}(\mh{n}; n', \eps)$.  So, the probability of a Type II error also goes to zero as $n,n' \conv \infty$.



% Importantly, the utility of any statistical test depends both on the p-value, the probability of obtaining a test statistic at least as extreme as the observed value (under the assumed model), and its power, the probability that the test will reject a false null hypothesis.  Ideally, the power of this test would go to unity, as $n,n' \rightarrow \infty$.  A sufficient condition for power to approach unity is that $g_n$ is a \emph{consistent} classifier.  



Unfortunately, the rate of convergence of $L_{F}(g_n)$ to $L_{F}(g^*)$ depends on the (unknown) distribution $F=F_{MB}$ \cite{DGL96}. Furthermore, arbitrarily slow convergence theorems regarding the rate of convergence of $L_{F}(g_n)$ to $L_{F}(g^*)$ demonstrate that there is no universal $n,n'$ which will guarantee that the test has power greater than any specified target $\beta > \alpha$ \cite{Devroye83}. For this reason, the test outlined above can provide only a one-sided conclusion: if we reject we can be $100(1-\alpha)$\% confident that $\MeB$ holds, but we can never be confident in its negation; rather, it may be the case that the evidence in favor of $\MeB$ is insufficient for any number of reasons, including that we simply have not yet collected enough data. Thus, without restrictions on $F_{MB}$, arbitrarily slow convergence theorems imply that our theorem of $\varepsilon$-supervenience does not \tr{strictly} satisfy Popper's {\it falsifiability} requirement \cite{Popper}.

Below, we show that under a very general mind-brain model, one can construct a consistent classifier whose power approaches unity with sufficient data.


% To ensure consistency and therefore unity power, the classifier $g_n$ must be able to converge to the truth, regardless of the true distribution, $F$.  We therefore make explicit a model for brains, and show that under this very general model, universally consistent classifiers are available.

\begin{thex}
Let the physical property under consideration be brain connectivity structure, so $b$ is a brain-graph (``connectome'') with vertices representing neurons (or neuroanatomical regions) and edges representing synapses (or white matter tracts). Further let $\mB$, the brain observation space, be the collection of all graphs on a finite number of vertices, and $\mc{M}$, the mental property observation space, be finite. Now, imagine collecting very large amounts of very accurate exchangeable  brain-graph data and the associated mental property indicators. A $k_n$-nearest neighbor classifier using an isomorphism-matching Frobenius norm is universally consistent (see Appendix 2 for proof sketch). Therefore, %Theorem \ref{thm1} applies and 
the existence of a universally consistent classifier guarantees that eventually (in $n,n'$) we will be able to conclude $\MeB$ for this mind-brain property pair, if indeed $\varepsilon$-supervenience holds. This logic holds for directed graphs or multigraphs or hypergraphs with discrete edge weights and vertex attributes. Furthermore, Appendix 2 also extends the proof to deal with other matrix norms (which might speed up convergence), and the regression scenario, where $|\mM|$ is infinite.  
\end{thex}


\section*{Discussion}

% \paragraph{Summary} % (fold)
% \label{par:summary}

We have introduced the notion of $\eps$-supervenience, which states that the Bayes optimal misclassification rate for any mind-brain property pair is less than $\eps$.  Furthermore, when we restrict the space of minds and brains to the setting of \emph{Gedankenexperiment}  1, we have shown that $k_n$-NN classifiers are universally consistent, such that one can derive a hypothesis test, with confidence level $\alpha$, that is guaranteed to converge to the Bayes optimal misclassification rate, given sufficient data, no matter the true (but unknown) distribution of mind-brain pair properties.  

Alas, this is a one-sided test, so although power converges to unity, %one can never determine the reason for a failure to reject. %one can never determine whether (i) more data is necessary to get a lower p-value, or (ii) that the particular $\eps$-supervenience does not hold.  
a failure to reject the null has many possible explanations.  First, the amount of data might be insufficient given the particular distance implemented; collecting more data or utilizing a more informative distance might resolve this difficulty.  Second, $\eps$-supervenience is defined between a mental property and a brain property.  Thus, a failure to reject supervenience on a particular brain property does not entail non-supervenience on any other brain property.  Third, in general, neither mental properties nor brain properties are directly measurable; rather, one typically measures a function of such properties.  For instance, instead of measuring intelligence, one considers the results of an IQ test as a proxy for intelligence.  Similarly, instead of measuring neural spike trains, one estimates spike trains from some measurable neural signal, like voltage.  While one could conceivably ask questions such as: ``is IQ score at some time supervenient on the the output of a voltage measuring device?'', these are less elegant and general than their non-observable analogs, such as: ``is intelligence supervenient on spike trains?''

Thus, given measurement of mental and brain properties that we believe reflect the properties of interest, and a sufficient amount data satisfying the exchangeability assumption, a rejection entails that we are $100(1-a)\%$ confident that the mental property under investigation does not supervene on the brain property under investigation.  Unfortunately, failure to reject is more ambiguous. $\eps-$supervenience tests can therefore be thought of as constraining the space of possible brain properties upon which a mental property supervenes.  Determining that a mental property does not supervene on \emph{any} brain property is beyond the capacities of this formalism.

Interestingly, much of contemporary research in neuroscience and cognitive science could be cast as mind-brain supervenience investigations.  Specifically, any investigation that aims to ``explain'' some behavioral or psychological phenomenon by reference to neural (or other brain) activity or structure, could be thought of as mind-brain supervenience investigations.  Thus, the proposed $\eps$-supervenience framework is perhaps a useful unifying perspective for these fields.  Perhaps this is especially true in light of the advent of ``connectomics'' \cite{SpornsKotter05,Hagmann05}, a field devoted to estimating whole organism brain-graphs, and relating them to function.  Testing supervenience of various mental properties on these brain-graphs will likely therefore become increasingly compelling; so the framework developed herein could be fundamental to these investigations.  Note that these connectomics investigations might lack any dynamics; rather, they effectively assume that structure determines the relevant dynamics.  Assuming that dynamics are a function of either vertex or edge attributes, whether any particular mental property is supervenient to ``only'' connectivity structure, or also some attributes, is merely another $\eps$-supervenience hypothesis. Appendix 3 presents an illustrative simulated example using the only currently known connectome---that of a Caenorhabditis elegans---demonstrating that $\eps$-supervenience can be rejected at some critical value $c_\alpha$ with only a reasonably small number of samples. Similar supervenience tests on larger animals will require either higher-throughput imaging modalities \cite{HayworthLichtman06, Bock2011} or more coarse brain-graphs \cite{PalmAmunts10,Johansen-Berg2009}, or both.




% \paragraph{Human applications} % (fold)
% \paragraph{Dynamics vs. statics} % (fold)
% \paragraph{Concluding thoughts} % (fold)
% \label{par:concluding_thoughts}

In conclusion, this \emph{Gedankenexperiment}, together with (i) the formal definition of $\varepsilon$-supervenience as a constraint on distributions, (ii) the brain-graph model, and (iii) the universal consistency proof on graphs, is the first demonstration (to our knowledge) that empirically investigating supervenience is at least theoretically possible. The above discussion suggests that many previously conducted investigations either assume supervenience, or try to test it.  Further, new technologies facilitate testing supervenience of mental properties on brain-graphs more easily.


\section*{Methods}
\label{sec:methods}
% \subsection*{Supervenience} % (fold)
% \label{sec:preliminaries}

% \subsection*{Mind Brain Supervenience} % (fold)
% \label{sub:supervenience}

% subsection supervenience (end)
% The intention in this work is to develop greater insight regarding the relationship between minds and brains, using statistical methods, with particular interest in notions of supervenience.  Modern philosophy of science suggests that adequate scientific explanations must account for the objects of investigation and their relationships \cite{Craver07}. Therefore, here we define minds, brains, and supervenience.
% The mind-brain supervenience conjecture is a relation between the set of mental states and the set of brain states.  


% \subsection*{Statistical Setting} % (fold)

% The approximate  number of misclassified minds therefore has a binomial distribution:  $n' \hL^{n'}_{F}(g_{n}) \sim \text{Binomial}(n',L_{F}(g_{n}))$.




\clearpage
\bibliography{/Users/jovo/Research/latex/library}
\bibliographystyle{nature}

\section*{Acknowledgments}

The authors would like to acknowledge helpful discussions with J.~Lande, B.~Vogelstein, and S.~Seung. 

\section*{Author Contributions}

JTV, RJV, and CEP conceived of the manuscript.  JTV and CEP wrote it.  CEP ran the experiment.

\section*{Additional Information}

The authors have no competing financial interests to declare.





\end{document}

