\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Relations between sets}{1}{section.1}}
\newlabel{sec:relations}{{1}{1}{Relations between sets\relax }{section.1}{}}
\citation{Stone1977}
\citation{DGL96}
\citation{GareyJohnson79}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Possible relations between minds and brains. (A) Minds supervene on brains, and it so happens that there is a bijective relation from brains to minds. (B) Minds supervene on brains, and it so happens that there is a surjective (a.k.a., onto) relation from brains to minds. (C) Minds are \emph  {not} supervenient on brains, because two different minds supervene on the same brain.}}{2}{figure.1}}
\newlabel{fig:rel}{{1}{2}{Possible relations between minds and brains. (A) Minds supervene on brains, and it so happens that there is a bijective relation from brains to minds. (B) Minds supervene on brains, and it so happens that there is a surjective (a.k.a., onto) relation from brains to minds. (C) Minds are \emph {not} supervenient on brains, because two different minds supervene on the same brain}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}$k_n$ nearest neighbor algorithm}{2}{section.2}}
\newlabel{sec:knn}{{2}{2}{$k_n$ nearest neighbor algorithm\relax }{section.2}{}}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\citation{Vaziri2008}
\citation{Helmstaedter2008}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{Buckingham2008}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulation}{3}{section.3}}
\newlabel{sec:sim}{{3}{3}{Simulation\relax }{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces C.\nobreakspace  {}elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{n'}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}})$ (the misclassification rate estimated upon with $n'=1000$ testing samples) is plotted as a function of class-conditional training sample size $n_j=\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}/2$, suggesting that for $\varepsilon =0.1$ we can determine that $\mathcal  {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_{\mathbb  {P}} \mathcal  {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{M,B}$. Each dot depicts an estimate for $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{n'}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}})$; standard errors are $(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{n'}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}}) (1-\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{n'}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}}))/n')^{1/2}$; e.g., $n_j = 180$; $k_n = 53$; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{n'}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}}) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation.}}{3}{figure.2}}
\newlabel{fig1}{{2}{3}{C.~elegans graph classification simulation results. $\hL ^{n'}_{F}(g_{\mt {n}})$ (the misclassification rate estimated upon with $n'=1000$ testing samples) is plotted as a function of class-conditional training sample size $n_j=\mt {n}/2$, suggesting that for $\varepsilon =0.1$ we can determine that $\MeB $ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{M,B}$. Each dot depicts an estimate for $\hL ^{n'}_{F}(g_{\mt {n}})$; standard errors are $(\hL ^{n'}_{F}(g_{\mt {n}}) (1-\hL ^{n'}_{F}(g_{\mt {n}}))/n')^{1/2}$; e.g., $n_j = 180$; $k_n = 53$; $\hL ^{n'}_{F}(g_{\mt {n}}) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation}{figure.2}{}}
\bibdata{/Users/jovo/Research/latex/library}
\bibcite{Stone1977}{1}
\bibcite{DGL96}{2}
\bibcite{GareyJohnson79}{3}
\bibcite{Durbin87}{4}
\bibcite{deBonoMaricq05}{5}
\bibcite{VarshneyChklovskii09}{6}
\bibcite{ChalasaniBargmann07}{7}
\bibcite{Vaziri2008}{8}
\bibcite{Helmstaedter2008}{9}
\bibcite{Mishchenko09}{10}
\bibcite{LuLichtman09}{11}
\bibcite{Buckingham2008}{12}
\bibstyle{nature}
