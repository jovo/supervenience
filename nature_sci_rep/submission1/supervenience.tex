%\input{/Users/joshyv/Research/misc/latex_paper.tex}
\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{times}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage[scaled]{helvet}
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\pagestyle{fancy}

\oddsidemargin=0.0in %%this makes the odd side margin go to the default of 1inch
\evensidemargin=0.0in
\textwidth=6.5in
\headwidth=6.5in
\textheight=9in %%sets the textwidth to 6.5, which leaves 1 for the remaining right margin with 8 1/2X11inch paper
\headheight=12pt
\topmargin=-0.25in
%\headheight=0in
%\headsep=0in
%\pagestyle{headings}

\usepackage{hyperref}
% \usepackage{ulem}
% \usepackage{color}

% \newcommand{\loo}{$L^{(1)}_{h; \mD_n}$}
\newcommand{\conv}{\rightarrow}
% \newcommand{\Real}{\mathbb{R}}
% \providecommand{\tr}[1]{\textcolor{red}{#1}}

\newcommand{\mB}{\mathcal{B}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mM}{\mathcal{M}}
\newcommand{\PP}{\mathbb{P}}           % probability
\newcommand{\EE}{\mathbb{E}}           % expected value
\newcommand{\II}{\mathbb{I}}           % expected value
\newcommand{\Real}{\mathbb{R}}           % expected value

\newcommand{\del}{\delta}
\newcommand{\sig}{\sigma}
\newcommand{\lam}{\lambda}
\newcommand{\gam}{\gamma}
\newcommand{\eps}{\varepsilon}

\providecommand{\mc}[1]{\mathcal{#1}}
\providecommand{\mb}[1]{\boldsymbol{#1}}
\providecommand{\mbb}[1]{\mathbb{#1}}
\providecommand{\mv}[1]{\vec{#1}}
\providecommand{\mh}[1]{\widehat{#1}}
\providecommand{\mt}[1]{\widetilde{#1}}
\providecommand{\mhc}[1]{\hat{\mathcal{#1}}}
\providecommand{\mhb}[1]{\hat{\boldsymbol{#1}}}
\providecommand{\mvb}[1]{\vec{\boldsymbol{#1}}}
\providecommand{\mtb}[1]{\widetilde{\boldsymbol{#1}}}

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}


% \newcommand{\mN}{\mathcal{N}}

\newcommand{\hL}{\widehat{L}}
\newcommand{\MeB}{\mM \overset{\varepsilon}{{\sim}}_{\PP} \mB}
\newcommand{\MsB}{\mM \overset{S}{\sim}_{\PP} \mB}
\newcommand{\MnoteB}{\mM \overset{\varepsilon}{{\not\sim}}_{\PP} \mB}
\providecommand{\tr}[1]{\textcolor{black}{#1}}
\providecommand{\norm}[1]{\left \lVert#1 \right  \rVert}
\newcommand{\T}{^{\ensuremath{\mathsf{T}}}}           % transpose

\newtheorem{defi}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{thex}{\emph{Gedankenexperiment}}
\lhead{Vogelstein JT, et al}
\rhead{Statistical Supervenience}


\begin{document}

	\begin{center}
{\huge	Are mental properties supervenient on brain properties?}
\end{center}

\vspace{5px}

\begin{center}
{\large		
Joshua T. Vogelstein$^{1*}$, R. Jacob Vogelstein$^2$, Carey E. Priebe$^1$\\
	$^1$Department of Applied Mathematics \& Statistics, \\ Johns Hopkins University, Baltimore, MD, 21218,\\ $^2$National Security Technology Department, \\ Johns Hopkins University Applied Physics Laboratory, Laurel, MD 20723}
	
\end{center}

\vspace{5px}

% \maketitle
% \date{\vspace{-5ex}}
%\tableofcontents
% \begin{abstract}
	
\noindent The``mind-brain supervenience'' conjecture suggests that all mental properties are derived from the physical properties of the brain. To address the question of whether the mind supervenes on the brain, we frame a supervenience hypothesis in rigorous statistical terms. Specifically, we propose a modified version of supervenience (called $\eps$-supervenience) that is amenable to experimental investigation and statistical analysis. To illustrate this approach, we perform a thought experiment that illustrates how the probabilistic theory of pattern recognition can be used to make a one-sided determination of $\eps$-supervenience. The physical property of the brain employed in this analysis is the graph describing brain connectivity (i.e., the brain-graph or connectome). $\eps$-supervenience allows us to determine whether a particular mental property can be inferred from one's connectome to within any given misclassification rate > 0, regardless of the relationship between the two. This may provide motivation for cross-disciplinary research between neuroscientists and statisticians.


% \end{abstract}

% \vspace*{0.5 in}

\newpage
% \section*{Introduction}

\noindent Questioning the relationship between the mind (our thoughts, beliefs, preferences, emotions, intelligences, etc.) and the brain (the physical structure inside our skulls) dates back at least as far as 400~BCE, when Plato wrote the dialogues, in which he posited immateriality of the soul \cite{Plato97}. Approximately two millennia passed before these ideas reached their canonical form through Descartes's discussion of mind-body dualism \cite{Descartes1641}. Then, in the 20th century, Donald Davidson stated and popularized the mind-brain supervenience \tr{conjecture}, which claims that an agent cannot alter in some mental property without altering in some physical property \cite{Davidson70}. Contemporary fields of neural network theory and neuroscientific inquiry often assume mind-brain supervenience, or an even stronger assumption about mind-brain causality, but no previously proposed notion of supervenience seems amenable to empirical investigation. This work is an attempt to bridge the gap between these philosophical conjectures and experimentally testable hypotheses. % Here we define new versions of supervenience that formulate the \tr{conjecture} in rigorous mathematical terms, and that can be experimentally tested \tr{as a hypothesis}.

The primary contributions of this work flow from our introduction of a notion of supervenience amenable to empirical investigation.  This renders the mind-brain dualism debate a hypothesis, rather than an assumption, both expanding the space of questions amenable to hypothesis testing, and placing limits on this space.  Because hypothesis tests (implicitly sometimes) depend on a model, a very general model of brains and their associated mental properties is proposed.  Fortunately, this formulation admits universally consistent classifiers, that is, classifiers guaranteed to find the relationship between minds and brains, if one exists, given sufficient data.  Many previous investigations relating brains and mental properties can therefore be considered $\eps$-supervenience hypothesis tests.  This paradigm, therefore, generalizes previous approaches, embedding them in a rigorous statistical framework, and suggests avenues for future research.


 % we demonstrate through simulation that the proposed universally consistent classifier has reasonable convergence properties on simulated brain-graph data. Finally, we discuss both contemporary and future applications of this framework to neurobiological studies in humans.


\section*{Results}

\subsection*{Statistical supervenience} % (fold)

Donald Davidson canonized the mind-brain supervenience relation in 1970 with the following quote: \cite{Davidson70}
\begin{quotation}
\noindent supervenience might be taken to mean that there cannot be two events alike in all physical respects but differing in some mental respect, or that an object cannot alter in some mental respect without altering in some physical respect.
\end{quotation}


\noindent This conjecture may be concisely and formally stated thusly:  $m \neq m' \implies b \neq b'$, where $(m,b), (m',b') \in \mc{M} \times \mc{B}$ are mind-brain pairs.  This mind-brain supervenience relation does not imply an injective relation, a causal relation, or an identity relation (see Appendix 1 for more details and some examples).  To facilitate both statistical analysis and empirical investigation, we convert this supervenience relation from a logical to a probabilistic relation.  

Let $b$ correspond to an agent's brain, which is a particular element from the set of all possible brains, $\mB$. Similarly, let $m$ correspond to an agent's mind, which is a particular element from the set of all possible minds, $\mM$.  Let $\PP[M,B]$ indicate a joint distribution of minds and brains. Statistical supervenience can thusly be defined:

\begin{defi}
\label{def1} 
$\mM$ is said to \textit{statistically supervene} on $\mB$ for distribution $\PP=\PP[M,B]$, denoted $\mM \overset{S}{\sim}_\PP \mB$, if and only if $\PP[m \neq m' | b=b']=0$, or equivalently $\PP[m = m' | b = b']=1$. 
\end{defi}
\noindent Statistical supervenience is therefore a probabilistic relation on sets (related to, but distinct from correlation; see Appendix 1 for details).  



\subsection*{Statistical supervenience is equivalent to perfect classification accuracy} % (fold)
\label{sub:theoretical_results}

If minds statistically supervene on brains, $\MsB$, then two different minds must supervene on two different brains.  This means that there exists a deterministic function mapping each brain to its supervening mind, $g(\cdot): \mB \mapsto \mM$; therefore, one could in principle construct this function. The optimal such classifier, $g^*$, has the smallest expected misclassification rate, $L_{\PP}(g^*)$.  The minimum misclassification rate is called Bayes error (see Methods for details). 
% It may be the case that subsets of brains form equivalence classes, such that any brain in that subset is mapped to the same mind.  
The primary result of casting supervenience as a statistical framework is the following theorem: 
\begin{thm}
\label{thm1} 
$\mM$ is said to \textit{statistically supervene} on $\mB$ for distribution $\PP=\PP[M,B]$, denoted $\mM \overset{S}{\sim}_{\PP} \mB$, if and only if $L_{\PP}(g^*) = 0$. Formally, \mbox{$\MsB \Leftrightarrow L_{\PP}(g^*)=0$}.  
\end{thm}

\noindent If minds supervene on brains, then, by the definition of supervenience, there exists a function that maps each brain deterministically to a particular mind.  This means that one could draw a decision boundary between all equivalence classes of brains, each class corresponding to a different mind, and no mind will reside within two different equivalence classes.  Thus, the optimal classifier would correctly find these decision boundaries, and therefore have no opportunity to err. $\square$

The above argument shows (for the first time to our knowledge) that statistical supervenience and zero Bayes error are equivalent. Statistical supervenience can therefore be thought of as a constraint on the possible distributions on minds and brains.  Specifically, let $\mc{P}$ indicate the set of all possible joint distributions on minds and brains, and let $\mc{P}_s$ be subset of distributions for which supervenience holds. Theorem \ref{thm1} implies that $\mc{P}_s = \{\PP[M,B] : L_{\PP}(g^*)=0\} \subseteq \mc{P}$.


\subsection*{$\eps$-supervenience admits hypothesis testing} % (fold)
\label{sub:hypothesis_testing}

Although the above theorem is of potential theoretical interest, the arguments rely on knowing the typically unknown $\PP[M,B]$ and $g^*$, rendering them useless pragmatically.  However, both $\PP[M,B]$ and $g^*$ could be estimated from data.  
Before explicitly considering the problem of testing for statistical supervenience, we define a relaxed notion of supervenience:
\begin{defi}
\label{def2}
Given $\varepsilon > 0$, $\mM$ is said to $\varepsilon$-\textit{supervene} on $\mB$ for distribution $\PP=\PP[M,B]$, denoted $\MeB$, if and only if $L_{\PP}(g^*) < \varepsilon$.
\end{defi}

Given this relaxation, consider the problem of testing for $\eps$-supervenience. Let the null hypothesis be $H_0$: $L_{\PP}(g_n) \geq \eps$, and the alternative hypothesis be $H_A$: $L_{\PP}(g_n) < \eps$. Hold-out error, $\hL^{n'}_{\PP}(g_{\mt{n}})$, as defined in Methods, is a test statistic for this hypothesis.  We reject for values of the test statistic lower than the critical value, that is, we reject if and only if $\hL^{n'}_{\PP}(g_{\mt{n}}) <c_{\alpha}(n',\varepsilon)$.  The critical value is available under the least favorable distribution $n' \hL^{n'}_{\PP}(g_{\mt{n}}) \sim \text{Binomial}(n',\varepsilon)$.  Thus, rejection implies that we are $100(1-\alpha$)\% confident that $\MeB$.  The definition of $\eps$-supervenience therefore admits, for the first time to our knowledge, a statistical test of supervenience, given a specified $\eps$ and $\alpha$. 

	
	
Importantly, the utility of any statistical test depends both on the p-value, the probability of obtaining a test statistic at least as extreme as the observed value (under the assumed model), and its power, the probability that the test will reject a false null hypothesis.  Ideally, the power of this test would go to unity, as $n,n' \rightarrow \infty$.  A sufficient condition for power to approach unity is that $g_n$ is a \emph{consistent} classifier.  A classifier is consistent if and only if its expected misclassification rate converges to the Bayes optimal limit with sufficient data, that is $\EE[L_{\PP}(g_n)] \conv L_{\PP}(g^*)$ as $n\conv \infty$.  Below, we show that under a very general mind-brain model, one can construct a consistent classifier whose power approaches unity with sufficient data.

Unfortunately, the rate of convergence of $L_{\PP}(g_n)$ to $L_{\PP}(g^*)$ depends on the (unknown) distribution $\PP=\PP[M,B]$ \cite{DGL96}. Furthermore, arbitrarily slow convergence theorems regarding the rate of convergence of $L_{\PP}(g_n)$ to $L_{\PP}(g^*)$ demonstrate that there is no universal $n,n'$ which will guarantee that the test has power greater than any specified target $\beta > \alpha$ \cite{Devroye83}. For this reason, the test outlined above can provide only a one-sided conclusion: if we reject we can be $100(1-\alpha)$\% confident that $\MeB$ holds, but we can never be confident in its negation; rather, it may be the case that the evidence in favor of $\MeB$ is insufficient for any number of reasons, including that we simply have not yet collected enough data. Thus, without restrictions on $\PP[M,B]$, arbitrarily slow convergence theorems imply that our theorem of $\varepsilon$-supervenience does not \tr{strictly} satisfy Popper's {\it falsifiability} requirement \cite{Popper}.


\subsection*{A \emph{Gedankenexperiment} demonstrating consistency and unity power} % (fold)
\label{sub:uc}

To ensure consistency and therefore unity power, the classifier $g_n(\cdot)$ must be able to converge to the truth, regardless of the true distribution, $\PP$.  We therefore make explicit a model for brains, and show that under this very general model, universally consistent classifiers are available.

\begin{thex}
Let the physical property under consideration be brain connectivity structure (``connectome''), so $b$ is a brain-graph \tr{(or, network)} with vertices representing neurons (or neuroanatomical regions) and edges representing synapses (or white matter tracts). Further let $\mB$, the observation space, be the collection of all graphs on a finite number of vertices, and let $|\mB|$ be countable. Now, imagine collecting very large amounts of very accurate independent and identically distributed brain-graph data and the associated mental property indicators. A $k_n$-nearest neighbor classifier using an isomorphism-matching Frobenius norm is universally consistent (see Appendix 2 for proof). Therefore, %Theorem \ref{thm1} applies and 
the existence of a universally consistent classifier guarantees that eventually (in $n,n'$) we will be able to conclude $\MeB$ for this mind/brain property pair, if indeed $\varepsilon$-supervenience holds. This logic holds for directed graphs or multigraphs or hypergraphs with discrete edge weights and vertex attributes. Furthermore, Appendix 2 also extends the proof to deal with other matrix norms (which might speed up convergence), and the regression scenario, where $|\mM|$ is infinite.  
\end{thex}


\section*{Discussion}

% \paragraph{Summary} % (fold)
% \label{par:summary}

We have introduced the notion of $\eps$-supervenience, which states that the Bayes optimal misclassification rate for any mind/brain property pair is less than $\eps$.  Furthermore, when we restrict the space of minds and brains to the setting of \emph{Gedankenexperiment}  1, we have shown that $k_n$-NN classifiers are universally consistent, such that one can derive a hypothesis test, with confidence level $\alpha$, that is guaranteed to converge to the Bayes optimal misclassification rate, given sufficient data, no matter the true (but unknown) distribution of mind/brain pair properties.  Alas, this is a one-sided test, so although power converges to unity, one can never determine whether (i) more data is necessary to get a lower p-value, or (ii) that the particular $\eps$-supervenience does not hold.  

% \paragraph{Practical issues} % (fold)
% \label{par:practical_issues}

Importantly, we are \emph{not} claiming that actually determining $\eps$-supervenience in humans is practically possible for any particular mental property at this time (at least when the vertices represent individual neurons).  Rather, the claim is that \emph{if} one had sufficient data (a very large number of exchangeable mind/brain property pairs), then \emph{in theory}, some level $\eps$-supervenience hypothesis test could be performed.  Even in such a scenario, a universally consistent classifier is just one of many possible kinds of classifiers, and not necessarily the best one (in terms of $\mh{L}$) for any particular dataset.  Further, even if a universally consistent classifier is used, a more informative and tractable distance on $\mB$ may be desired, as the $k_n$-nearest neighbor classifier under a Frobenius norm may have a rate of convergence so slow and a computational demand so high as to be impractical (but see Appendix 3 for a simulated example in which convergence is relatively fast).  Whichever classifier is used, it is likely to benefit from a large amount of domain-specific knowledge, which the proposed classifier completely neglects. 


% \paragraph{A unified quest} % (fold)
% \label{sub:practical_applications}

A central (perhaps \emph{the} central) quest in much of neuroscience, psychology, and cognitive science is to discover the brain properties that subvene under various mental properties, although questions are rarely cast within a  supervenience formalism.  Moreover, the particular brain properties that are often believed to subvene under these mental properties are neural circuits, or brain-subgraphs.  To this end, many investigations in these fields include schematic diagrams showing a particular brain-subgraph subvening under a particular mental phenotype. This practice transcends the evolutionary hierarchy of neuroscientific research.  For instance, in the invertebrate literature, vertices correspond to particular labeled neurons, and edges correspond to synapses \cite{NorthGreenspan07}.  In the vertebrate literature, vertices often correspond to types of neurons in particular regions, and edges correspond to tendencies of connections \cite{Shepherd04}.  For primates \cite{Felleman_VanEssen91} and humans \cite{Mori05} vertices frequently represent functionally distinct neuroanatomical regions, and edges represent regional interconnectivity. Furthermore, this practice also transcends analytical background, including anatomists \cite{Abeles91}, philosophers \cite{Koch_Davis94}, statisticians \cite{Rao_Lewicki02}, and physicists \cite{Chow_Dalibard03}.  The near ubiquity of this practice suggests that a fundamental quest is to determine which brain-subgraphs subvene under which mental properties (although perhaps causality, not supervenience, is the true desideratum).  Perhaps supervenience is therefore a framework that can fruitfully be applied to myriad and varied neurcognitive investigations.



% \paragraph{Human applications} % (fold)
% \label{par:human_applications}

In recent years, with the advent of the field of ``connectomics'' \cite{SpornsKotter05,Hagmann05}, neuroimaging has driven an explosion of studies investigating the human connectome and relating connectomes to cognitive properties \cite{Sporns10}. % \cite{Jones10}.  
Many of these studies can be framed as $\eps$-supervenience hypothesis tests.  For instance, a recent study showed that using data from diffusion tensor imaging \cite{Basser94}, one can nearly perfectly differentiate between schizophrenic individuals and normal (control) individuals \cite{ArdekaniSzeszko10}.  As the resolution and signal-to-noise ratio of magnetic resonance imaging continue to improve, especially with more advanced techniques such as High Angular Diffusion Imaging \cite{TuchWedeen02}, Q-Ball Imaging \cite{Tuch04}, and diffusion spectrum imaging \cite{WedeenWeisskoff05}, similar results could be obtained with other, more subtle cognitive properties.  Furthermore, the utilization of other imaging technologies, such as polarized light imaging \cite{PalmAmunts10} and high-throughput electron microscopy \cite{DenkHorstmann04,HayworthLichtman06}, will continue to improve the effective resolution of these inferred connectomes from human brains. While determining from a brain scan whether a particular individual knows calculus might be quite distant, many other cognitive and psychological supervenience hypotheses have already been tested, and the gap between testing for calculus and testing for schizophrenia seems to be diminishing.



% \paragraph{Alternative explanations} % (fold)

Although hypothesis testing for a particular $\epsilon$-supervenience appears to be possible based on the \emph{Gedankenexperiment} in Results, a natural question to raise is: what are the conceivable alternative hypotheses?  We consider three such alternatives.  

First, perhaps brains are more accurately characterized as quantum networks over classical networks.  Several authors have suggested that brains have certain hypercomputational properties that classical computers could not achieve \cite{Penrose99,Satinover02}. However, assuming that the computer can be represented as a network, the above results hold regardless of whether computations in the brain are quantum or classical.  This follows because quantum networks merely speed up computation for certain classes of problems; they cannot, however, solve problems that classical computers cannot \cite{NielsenChuang00}.  This means that if the above analysis failed to reject the null hypothesis at level $\alpha$, it will fail regardless of whether one assumes quantum or classical computations.

Second, perhaps minds stochastically supervene on brains \cite{Craver09}. While perhaps difficult to imagine, 
much like a non-deterministic world was difficult to imagine prior to modern physics, it is not inconceivable that mental properties are only stochastically determined by physical ones.

A third alternative hypothesis is supernatural causal effects.  The above analysis could be considered an empirical test for whether we have souls, or, perhaps whether souls play a causal role in our mental properties over and above the physical role played by the brain, or whether the data we have suggests that the probability that our souls play a measurable causal role over and above the physical is less than $\eps$.

It therefore seems that failing to reject the null hypothesis that a particular mental property $\eps$-supervenes on a particular brain property could potentially be explained by stochastic or supernatural forces, but not a quantum network brain model.



% \paragraph{Dynamics vs. statics} % (fold)
% \label{par:dynamics_vs_statics}

The \emph{Gedankenexperiment} in Section \ref{sub:uc} did not require simulating any dynamics; rather, the dynamics are necessarily a function of the model parameters (statics).  Similarly, for the question of mind-brain supervenience in humans, one need not ever observe any activity of the brain, one must merely observe the model that determines the activity (in a potentially stochastic process). Thus, this approach to understanding the relationship between mind and brain is distinct from the standard systems neuroscience paradigm, in which the goal is typically to understand the neural activity ``code.''  In contrast, if mind-brain supervenience holds, it motivates a search for the neural \emph{connectivity} ``code,'' a so-called ``engram''  for memories \cite{Semon21, Lashley50, ZhangLinden03, ShemaDudai07, BerryDavis08}, or more generally a \emph{mengram}, the neural signature of any mental property, be it cognitive, psychological, or otherwise (note that supervenience allows for the particular mengram of a mental property to vary both across individuals and time).  


% \paragraph{Concluding thoughts} % (fold)
% \label{par:concluding_thoughts}


This \emph{Gedankenexperiment}, together with (i) the formal definition of $\varepsilon$-supervenience as a constraint on distributions, (ii) the brain-graph model, and (iii) the universal consistency proof on graphs, is the first demonstration (to our knowledge) that empirically investigating supervenience is at least theoretically possible. The above discussion suggests that many previously conducted investigations either assume supervenience, or test it.  Further, new technologies facilitate testing supervenience of mental properties on brain-graphs more easily.


\section*{Methods}
\label{sec:methods}
% \subsection*{Supervenience} % (fold)
% \label{sec:preliminaries}

% \subsection*{Mind Brain Supervenience} % (fold)
% \label{sub:supervenience}

% subsection supervenience (end)
% The intention in this work is to develop greater insight regarding the relationship between minds and brains, using statistical methods, with particular interest in notions of supervenience.  Modern philosophy of science suggests that adequate scientific explanations must account for the objects of investigation and their relationships \cite{Craver07}. Therefore, here we define minds, brains, and supervenience.
% The mind-brain supervenience conjecture is a relation between the set of mental states and the set of brain states.  


% \subsection*{Statistical Setting} % (fold)


Assuming for the moment that the space of all possible minds is finite, that is, $|\mM| < \infty$, then we call any such function a \emph{classifier} (this assumption will later be relaxed).  Let $\mh{m}$ denote the output of a classifier, $g(b)=\mh{m}$.  Define misclassification rate as $L_{\PP}(g) = \PP[g(B) \neq M]$ which denotes the probability that $g$ misclassifies $b$. The Bayes optimal classifier $g^*$ minimizes $L_{\PP}(g)$ over all classifiers, that is:	
$g^* = \argmin_{g} L_{\PP}(g)$.
Thus, the \emph{Bayes error}, or Bayes risk, $L_{\PP}(g^*)$ is the minimum possible misclassification rate.

Let $\mc{T}_n=\{(m_1,b_1), (m_2,b_2), \ldots, (m_n,b_n)\}$ be a set of random samples taking their values in $\mc{M} \times \mc{B}$, each independently and identically distributed according to $\PP[M,B]$.  Generalizing the concept of a classifier $g$ to allow incorporation of training data, consider $g_n:\mB \times (\mc{M} \times \mc{B})^n \mapsto \mM$ which takes as input an observed brain  $b$ and training data $\mc{T}_n$, and produces a classification: $g_n(b;\tr{\mc{T}_n})=\mh{m}$.  Misclassification rate for this classifier will be a random variable, because the training data $\mc{T}_n$ are random samples.  The expected misclassification rate for this classifier is therefore approximated by ``hold-out'' error:
$\hL^{n'}_{\PP}(g_{\mt{n}}) = \PP[g_{\mt{n}}(B)=M | \mc{T}_{\mt{n}}]$,
where $\mt{n}=n-n'$, and $n'<n$ is the number of held-out training samples (samples not used to obtain $g_{\mt{n}}(\cdot)$).
The approximate  number of misclassified minds therefore has a binomial distribution:  $n' \hL^{n'}_{\PP}(g_{\mt{n}}) \sim \text{Binomial}(n',L_{\PP}(g_{\mt{n}}))$.




\clearpage

% subsection simulation (end)

\bibliography{/Users/jovo/Research/latex/biblist}
\bibliographystyle{nature}

% \begin{thebibliography}{10}
% \expandafter\ifx\csname url\endcsname\relax
%   \def\url#1{\texttt{#1}}\fi
% \expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
% \providecommand{\bibinfo}[2]{#2}
% \providecommand{\eprint}[2][]{\url{#2}}
% 
% \bibitem{Plato97}
% \bibinfo{author}{Plato}.
% \newblock \emph{\bibinfo{title}{Plato: complete works}}
%   (\bibinfo{publisher}{Hackett Pub Co}, \bibinfo{year}{1997}).
% 
% \bibitem{Descartes1641}
% \bibinfo{author}{Descartes, R.}
% \newblock \emph{\bibinfo{title}{Meditationes de prima philosophia}}
%   (\bibinfo{year}{1641}).
% 
% \bibitem{Davidson70}
% \bibinfo{author}{Davidson, D.}
% \newblock \emph{\bibinfo{title}{Experience and Theory}}, chap.
%   \bibinfo{chapter}{Mental Events} (\bibinfo{publisher}{Duckworth},
%   \bibinfo{year}{1970}).
% 
% \bibitem{DGL96}
% \bibinfo{author}{Devroye, L.}, \bibinfo{author}{Gy{\"o}rfi, L.} \&
%   \bibinfo{author}{Lugosi, G.}
% \newblock \emph{\bibinfo{title}{{A Probabilistic Theory of Pattern
%   Recognition}}} (\bibinfo{publisher}{Springer}, \bibinfo{year}{1996}).
% 
% \bibitem{Devroye83}
% \bibinfo{author}{Devroye, L.}
% \newblock \bibinfo{title}{{On arbitrarily slow rates of global convergence in
%   density estimation}}.
% \newblock \emph{\bibinfo{journal}{Probability Theory and Related Fields}}
%   \textbf{\bibinfo{volume}{62}}, \bibinfo{pages}{475--483}
%   (\bibinfo{year}{1983}).
% 
% \bibitem{Popper}
% \bibinfo{author}{Popper, K.}
% \newblock \bibinfo{title}{{The logic of scientific discovery}}
%   (\bibinfo{year}{1959}).
% 
% \bibitem{NorthGreenspan07}
% \bibinfo{editor}{Geoffrey~North, R. J.~G.} (ed.)
%   \emph{\bibinfo{title}{Invertebrate neurobiology}} (\bibinfo{publisher}{CSHL
%   Press}, \bibinfo{year}{2007}).
% 
% \bibitem{Shepherd04}
% \bibinfo{author}{Shepherd, G.}
% \newblock \emph{\bibinfo{title}{{The synaptic organization of the brain}}}
%   (\bibinfo{publisher}{Oxford University Press New York},
%   \bibinfo{year}{2004}).
% 
% \bibitem{Felleman_VanEssen91}
% \bibinfo{author}{Felleman, D.} \& \bibinfo{author}{Van~Essen, D.}
% \newblock \bibinfo{title}{{Distributed hierarchical processing in the primate
%   cerebral cortex}}.
% \newblock \emph{\bibinfo{journal}{Cerebral cortex}}
%   \textbf{\bibinfo{volume}{1}}, \bibinfo{pages}{1} (\bibinfo{year}{1991}).
% 
% \bibitem{Mori05}
% \bibinfo{author}{S.~Mori, P. v. Z. L. N.-P., S.~Wakana}.
% \newblock \emph{\bibinfo{title}{MRI Atlas of Human White Matter}}
%   (\bibinfo{publisher}{Elsevier Science}, \bibinfo{year}{2005}).
% 
% \bibitem{Abeles91}
% \bibinfo{author}{Abeles, M.}
% \newblock \emph{\bibinfo{title}{Corticonics}} (\bibinfo{publisher}{Cambridge
%   University Press}, \bibinfo{year}{1991}).
% 
% \bibitem{Koch_Davis94}
% \bibinfo{author}{Koch, C.} \& \bibinfo{author}{Davis, J.}
% \newblock \emph{\bibinfo{title}{{Large-scale neuronal theories of the brain}}}
%   (\bibinfo{publisher}{The MIT Press}, \bibinfo{year}{1994}).
% 
% \bibitem{Rao_Lewicki02}
% \bibinfo{author}{Rao, R.}, \bibinfo{author}{Olshausen, B.} \&
%   \bibinfo{author}{Lewicki, M.}
% \newblock \emph{\bibinfo{title}{{Probabilistic models of the brain: Perception
%   and neural function}}} (\bibinfo{publisher}{The MIT Press},
%   \bibinfo{year}{2002}).
% 
% \bibitem{Chow_Dalibard03}
% \bibinfo{editor}{Chow, C.}, \bibinfo{editor}{Gutkin, B.},
%   \bibinfo{editor}{Hansel, D.}, \bibinfo{editor}{Meunier, C.} \&
%   \bibinfo{editor}{Dalibard, J.} (eds.) \emph{\bibinfo{title}{Methods and
%   Models in Neurophysics}} (\bibinfo{publisher}{Elsevier},
%   \bibinfo{year}{2003}).
% 
% \bibitem{SpornsKotter05}
% \bibinfo{author}{Sporns, O.}, \bibinfo{author}{Tononi, G.} \&
%   \bibinfo{author}{Kotter, R.}
% \newblock \bibinfo{title}{The human connectome: A structural description of the
%   human brain}.
% \newblock \emph{\bibinfo{journal}{PLoS Computational Biology}}
%   \textbf{\bibinfo{volume}{1}}, \bibinfo{pages}{e42} (\bibinfo{year}{2005}).
% 
% \bibitem{Hagmann05}
% \bibinfo{author}{Hagmann, P.}
% \newblock \emph{\bibinfo{title}{{From diffusion MRI to brain connectomics}}}.
% \newblock Ph.D. thesis, \bibinfo{school}{Institut de traitement des signaux}
%   (\bibinfo{year}{2005}).
% 
% \bibitem{Sporns10}
% \bibinfo{author}{Sporns, O.}
% \newblock \emph{\bibinfo{title}{Networks of the Brain}}
%   (\bibinfo{publisher}{MIT Press}, \bibinfo{year}{2010}).
% 
% \bibitem{Basser94}
% \bibinfo{author}{Basser, P.~J.}, \bibinfo{author}{Mattiello, J.} \&
%   \bibinfo{author}{LeBihan, D.}
% \newblock \bibinfo{title}{Mr diffusion tensor spectroscopy and imaging.}
% \newblock \emph{\bibinfo{journal}{Biophys J}} \textbf{\bibinfo{volume}{66}},
%   \bibinfo{pages}{259--267} (\bibinfo{year}{1994}).
% \newblock \urlprefix\url{http://dx.doi.org/10.1016/S0006-3495(94)80775-1}.
% 
% \bibitem{ArdekaniSzeszko10}
% \bibinfo{author}{Ardekani, B.~A.} \emph{et~al.}
% \newblock \bibinfo{title}{Diffusion tensor imaging reliably differentiates
%   patients with schizophrenia from healthy volunteers.}
% \newblock \emph{\bibinfo{journal}{Hum Brain Mapp}}  (\bibinfo{year}{2010}).
% \newblock \urlprefix\url{http://dx.doi.org/10.1002/hbm.20995}.
% 
% \bibitem{TuchWedeen02}
% \bibinfo{author}{Tuch, D.} \emph{et~al.}
% \newblock \bibinfo{title}{{High angular resolution diffusion imaging reveals
%   intravoxel white matter fiber heterogeneity}}.
% \newblock \emph{\bibinfo{journal}{Magnetic Resonance in Medicine}}
%   \textbf{\bibinfo{volume}{48}}, \bibinfo{pages}{577--582}
%   (\bibinfo{year}{2002}).
% 
% \bibitem{Tuch04}
% \bibinfo{author}{Tuch, D.}
% \newblock \bibinfo{title}{{Q-ball imaging}}.
% \newblock \emph{\bibinfo{journal}{Magnetic Resonance in Medicine}}
%   \textbf{\bibinfo{volume}{52}}, \bibinfo{pages}{1358--1372}
%   (\bibinfo{year}{2004}).
% 
% \bibitem{WedeenWeisskoff05}
% \bibinfo{author}{Wedeen, V.}, \bibinfo{author}{Hagmann, P.},
%   \bibinfo{author}{Tseng, W.}, \bibinfo{author}{Reese, T.} \&
%   \bibinfo{author}{Weisskoff, R.}
% \newblock \bibinfo{title}{{Mapping complex tissue architecture with diffusion
%   spectrum magnetic resonance imaging}}.
% \newblock \emph{\bibinfo{journal}{Magnetic Resonance in Medicine}}
%   \textbf{\bibinfo{volume}{54}}, \bibinfo{pages}{1377--1386}
%   (\bibinfo{year}{2005}).
% 
% \bibitem{PalmAmunts10}
% \bibinfo{author}{Palm, C.} \emph{et~al.}
% \newblock \bibinfo{title}{{Towards ultra-high resolution fibre tract mapping of
%   the human brain--registration of polarised light images and reorientation of
%   fibre vectors}}.
% \newblock \emph{\bibinfo{journal}{Frontiers in Human Neuroscience}}
%   \textbf{\bibinfo{volume}{4}} (\bibinfo{year}{2010}).
% 
% \bibitem{DenkHorstmann04}
% \bibinfo{author}{W.~Denk, W.} \& \bibinfo{author}{Horstmann, H.}
% \newblock \bibinfo{title}{Serial block-face scanning electron microscopy to
%   reconstruct three-dimensional tissue nanostructure}.
% \newblock \emph{\bibinfo{journal}{PLOS Biol.}} \textbf{\bibinfo{volume}{2}},
%   \bibinfo{pages}{e329} (\bibinfo{year}{2004}).
% 
% \bibitem{HayworthLichtman06}
% \bibinfo{author}{Hayworth, K.}, \bibinfo{author}{Kasthuri, N.},
%   \bibinfo{author}{Schalek, R.} \& \bibinfo{author}{Lichtman, J.}
% \newblock \bibinfo{title}{{Automating the collection of ultrathin serial
%   sections for large volume TEM reconstructions}}.
% \newblock \emph{\bibinfo{journal}{Microscopy and Microanalysis}}
%   \textbf{\bibinfo{volume}{12}}, \bibinfo{pages}{86--87}
%   (\bibinfo{year}{2006}).
% 
% \bibitem{Penrose99}
% \bibinfo{author}{Penrose, R.} \& \bibinfo{author}{Gardner, M.}
% \newblock \emph{\bibinfo{title}{{The emperor's new mind: concerning computers,
%   minds, and the laws of physics}}} (\bibinfo{publisher}{Oxford University
%   Press, USA}, \bibinfo{year}{1999}).
% 
% \bibitem{Satinover02}
% \bibinfo{author}{Satinover, J.}
% \newblock \emph{\bibinfo{title}{{The quantum brain: the search for freedom and
%   the next generation of man}}} (\bibinfo{publisher}{Wiley},
%   \bibinfo{year}{2002}).
% 
% \bibitem{NielsenChuang00}
% \bibinfo{author}{Nielson, M.~A.} \& \bibinfo{author}{Chuang, I.~L.}
% \newblock \emph{\bibinfo{title}{Quantum Computation and Quantum Information}}
%   (\bibinfo{publisher}{Cambridge University Press}, \bibinfo{year}{2000}).
% 
% \bibitem{Craver09}
% \bibinfo{author}{Craver, C.~F.}
% \newblock \bibinfo{title}{Stochastic supervenience}  (\bibinfo{year}{2009}).
% 
% \bibitem{Semon21}
% \bibinfo{author}{Semon, R.~W.}
% \newblock \emph{\bibinfo{title}{The Mneme}} (\bibinfo{publisher}{G. Allen \&
%   Unwin ltd.}, \bibinfo{year}{1921}).
% 
% \bibitem{Lashley50}
% \bibinfo{author}{Lashley, K.}
% \newblock \bibinfo{title}{{In search of the engram}}.
% \newblock \emph{\bibinfo{journal}{Symposia of the society for experimental
%   biology}} \textbf{\bibinfo{volume}{4}}, \bibinfo{pages}{30}
%   (\bibinfo{year}{1950}).
% 
% \bibitem{ZhangLinden03}
% \bibinfo{author}{Zhang, W.} \& \bibinfo{author}{Linden, D.}
% \newblock \bibinfo{title}{{The other side of the engram: experience-driven
%   changes in neuronal intrinsic excitability}}.
% \newblock \emph{\bibinfo{journal}{Nature Reviews Neuroscience}}
%   \textbf{\bibinfo{volume}{4}}, \bibinfo{pages}{885--900}
%   (\bibinfo{year}{2003}).
% 
% \bibitem{ShemaDudai07}
% \bibinfo{author}{Shema, R.}, \bibinfo{author}{Sacktor, T.} \&
%   \bibinfo{author}{Dudai, Y.}
% \newblock \bibinfo{title}{{Rapid erasure of long-term memory associations in
%   the cortex by an inhibitor of PKM $\{$zeta$\}$}}.
% \newblock \emph{\bibinfo{journal}{Science}} \textbf{\bibinfo{volume}{317}},
%   \bibinfo{pages}{951} (\bibinfo{year}{2007}).
% 
% \bibitem{BerryDavis08}
% \bibinfo{author}{Berry, J.}, \bibinfo{author}{Krause, W.} \&
%   \bibinfo{author}{Davis, R.}
% \newblock \bibinfo{title}{{Olfactory memory traces in Drosophila}}.
% \newblock \emph{\bibinfo{journal}{Progress in brain research}}
%   \textbf{\bibinfo{volume}{169}}, \bibinfo{pages}{293--304}
%   (\bibinfo{year}{2008}).
% 
% \end{thebibliography}




\section*{Acknowledgments}

The authors would like to acknowledge helpful discussions with J.~Lande, B.~Vogelstein, and S.~Seung. 

\section*{Author Contributions}

JTV, RJV, and CEP conceived of the manuscript.  JTV and CEP wrote it.  CEP ran the experiment.

\section*{Additional Information}

The authors have no competing financial interests to declare.





\end{document}

