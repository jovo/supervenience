\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Relations between sets}{1}{section.1}}
\newlabel{sec:relations}{{1}{1}{Relations between sets\relax }{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}$k_n$ nearest neighbor algorithm}{1}{section.2}}
\newlabel{sec:knn}{{2}{1}{$k_n$ nearest neighbor algorithm\relax }{section.2}{}}
\citation{DGL96}
\citation{GareyJohnson79}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Possible relations between minds and brains. (A) Minds supervene on brains, and it so happens that there is a bijective relation from brains to minds. (B) Minds supervene on brains, and it so happens that there is a surjective (a.k.a., onto) relation from brains to minds. (C) Minds are \emph  {not} supervenient on brains, because two different minds supervene on the same brain.}}{2}{figure.1}}
\newlabel{fig:rel}{{1}{2}{Possible relations between minds and brains. (A) Minds supervene on brains, and it so happens that there is a bijective relation from brains to minds. (B) Minds supervene on brains, and it so happens that there is a surjective (a.k.a., onto) relation from brains to minds. (C) Minds are \emph {not} supervenient on brains, because two different minds supervene on the same brain}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Simulation}{2}{section.3}}
\newlabel{sec:sim}{{3}{2}{Simulation\relax }{section.3}{}}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\citation{VaziriShank08}
\citation{HelmstaedterDenk08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{BuckinghamSattelle08}
\bibdata{/Users/jovo/Research/latex/biblist}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces C.\nobreakspace  {}elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}$ (the misclassification rate estimated upon with $1000$ testing samples) is plotted as a function of class-conditional training sample size $n_j=\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}/2$, suggesting that for $\varepsilon =0.1$ we can determine that $\mathcal  {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_{\mathbb  {P}} \mathcal  {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $\mathbb  {P}[M,B]$. Each dot depicts an estimate for $L_{\mathbb  {P}}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}})$; standard errors are $(L_{\mathbb  {P}}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}})(1-L_{\mathbb  {P}}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}}))/1000)^{1/2}$\leavevmode {\color  {black}; e}.g., $n_j = 180$; $k_n = 53$; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle n$}\mathaccent "0365{n}}) = 0.057$; standard error less than 0.01. We reject $H_0: L_{\mathbb  {P}}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{\mathbb  {P}}(g^*) \approx 0$ for this simulation.}}{3}{figure.2}}
\newlabel{fig1}{{2}{3}{C.~elegans graph classification simulation results. $\hL $ (the misclassification rate estimated upon with $1000$ testing samples) is plotted as a function of class-conditional training sample size $n_j=\mt {n}/2$, suggesting that for $\varepsilon =0.1$ we can determine that $\MeB $ holds with $99\%$ confidence with just a few hundred training samples generated from $\PP [M,B]$. Each dot depicts an estimate for $L_{\PP }(g_{\mt {n}})$; standard errors are $(L_{\PP }(g_{\mt {n}})(1-L_{\PP }(g_{\mt {n}}))/1000)^{1/2}$\tr {; e}.g., $n_j = 180$; $k_n = 53$; $\hL ^{1000}_{F}(g_{\mt {n}}) = 0.057$; standard error less than 0.01. We reject $H_0: L_{\PP }(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{\PP }(g^*) \approx 0$ for this simulation}{figure.2}{}}
\bibcite{DGL96}{1}
\bibcite{GareyJohnson79}{2}
\bibcite{Durbin87}{3}
\bibcite{deBonoMaricq05}{4}
\bibcite{VarshneyChklovskii09}{5}
\bibcite{ChalasaniBargmann07}{6}
\bibcite{VaziriShank08}{7}
\bibcite{HelmstaedterDenk08}{8}
\bibcite{Mishchenko09}{9}
\bibcite{LuLichtman09}{10}
\bibcite{BuckinghamSattelle08}{11}
\bibstyle{nature}
