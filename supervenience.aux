\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{Plato97}
\citation{Descartes1641}
\citation{Davidson70}
\citation{SpornsKotter05}
\citation{LichtmanSanes08}
\citation{Seung09}
\citation{Kim95}
\citation{DGL96}
\citation{Devroye83}
\citation{Popper}
\newlabel{def1}{{1}{3}{\relax }{defi.1}{}}
\newlabel{def2}{{2}{3}{\relax }{defi.2}{}}
\newlabel{thm1}{{1}{3}{\relax }{thm.1}{}}
\citation{WhiteBrenner86}
\citation{DenkHorstmann04}
\citation{BriggmanDenk06}
\citation{MackeBorst08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{HornJohnson90}
\citation{ConroyLouis97}
\citation{ZaslavskiyVert08}
\citation{DGL96}
\citation{Stone77}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\newlabel{proof}{{}{5}{Appendix 1: $k_n$-nearest neighbor universal consistency for graphs\relax }{section*.1}{}}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\citation{VaziriShank08}
\citation{HelmstaedterDenk08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{BuckinghamSattelle08}
\bibdata{biblist}
\bibcite{Plato97}{1}
\bibcite{Descartes1641}{2}
\bibcite{Davidson70}{3}
\newlabel{simulation}{{}{6}{Appendix 2: Simulation\relax }{section*.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgments}{6}{section*.3}}
\bibcite{SpornsKotter05}{4}
\bibcite{LichtmanSanes08}{5}
\bibcite{Seung09}{6}
\bibcite{Kim95}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces C.\nobreakspace  {}elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n)$ is plotted as a function of class-conditional training sample size $n_j$, suggesting that for $\varepsilon =0.1$ we can determine that $\@mathcal {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_F \@mathcal {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{BM}$. Each dot depicts an estimate for $L_{F}(g_n)$; standard errors are $(L_{F}(g_n)(1-L_{F}(g_n))/1000)^{1/2}$\leavevmode {\color  {black}; e}.g., $n_j = 180$ ; $k_n = 53$ ; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation. }}{7}{figure.1}}
\newlabel{fig1}{{1}{7}{Appendix 2: Simulation\relax }{figure.1}{}}
\bibcite{DGL96}{8}
\bibcite{Devroye83}{9}
\bibcite{Popper}{10}
\bibcite{WhiteBrenner86}{11}
\bibcite{DenkHorstmann04}{12}
\bibcite{BriggmanDenk06}{13}
\bibcite{MackeBorst08}{14}
\bibcite{Mishchenko09}{15}
\bibcite{LuLichtman09}{16}
\bibcite{HornJohnson90}{17}
\bibcite{ConroyLouis97}{18}
\bibcite{ZaslavskiyVert08}{19}
\bibcite{Stone77}{20}
\bibcite{Durbin87}{21}
\bibcite{deBonoMaricq05}{22}
\bibcite{VarshneyChklovskii09}{23}
\bibcite{ChalasaniBargmann07}{24}
\bibcite{VaziriShank08}{25}
\bibcite{HelmstaedterDenk08}{26}
\bibcite{BuckinghamSattelle08}{27}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.4}}
