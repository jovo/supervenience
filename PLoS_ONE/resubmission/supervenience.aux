\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{Plato97}
\citation{Descartes1641}
\citation{Davidson70}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Davidson70}
\@writefile{toc}{\contentsline {section}{\numberline {2}Preliminaries}{2}{section.2}}
\newlabel{sec:preliminaries}{{2}{2}{Preliminaries\relax }{section.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Possible relations between minds and brains. (A) Minds supervene on brains, and it so happens that there is a bijective relation from brains to minds. (B) Minds supervene on brains, and it so happens that there is a surjective relation from brains to minds. (C) Minds are \emph  {not} supervenient on brains, because two different minds supervene on the same brain.}}{3}{figure.1}}
\newlabel{fig:rel}{{1}{3}{Preliminaries\relax }{figure.1}{}}
\newlabel{def1}{{1}{3}{Preliminaries\relax }{defi.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Theoretical results}{3}{subsection.3.1}}
\newlabel{sub:theoretical_results}{{3.1}{3}{Theoretical results\relax }{subsection.3.1}{}}
\newlabel{thm1}{{1}{4}{Theoretical results\relax }{thm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Hypothesis testing}{4}{subsection.3.2}}
\newlabel{sub:hypothesis_testing}{{3.2}{4}{Hypothesis testing\relax }{subsection.3.2}{}}
\citation{DGL96}
\citation{Devroye83}
\citation{Popper}
\citation{Stone77}
\citation{Cybenko89}
\citation{Waldeyer-Hartz1891}
\citation{Finger01}
\citation{Bishop95}
\citation{McClellandRumelhart86}
\citation{Bollobas}
\citation{otherbook}
\newlabel{def2}{{2}{5}{Hypothesis testing\relax }{defi.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Power and consistency}{5}{subsection.3.3}}
\newlabel{ssub:power_and_consistency}{{3.3}{5}{Power and consistency\relax }{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Brain-graphs}{5}{subsection.3.4}}
\newlabel{sub:brain_graphs}{{3.4}{5}{Brain-graphs\relax }{subsection.3.4}{}}
\citation{WhiteBrenner86}
\citation{DenkHorstmann04}
\citation{BriggmanDenk06}
\citation{MackeBorst08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\@writefile{toc}{\contentsline {section}{\numberline {4}Old results}{6}{section.4}}
\newlabel{sec:old_results}{{4}{6}{Old results\relax }{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{6}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {A}$k_n$ nearest neighbor algorithm}{6}{section.A}}
\newlabel{app:knn}{{A}{6}{$k_n$ nearest neighbor algorithm\relax }{section.A}{}}
\citation{DGL96}
\citation{HornJohnson90}
\citation{ConroyLouis97}
\citation{ZaslavskiyVert08}
\citation{DGL96}
\citation{Stone77}
\citation{Durbin87}
\citation{Durbin87}
\citation{deBonoMaricq05}
\@writefile{toc}{\contentsline {section}{\numberline {B}$k_n$-nearest neighbor universal consistency for graphs}{7}{section.B}}
\newlabel{proof}{{B}{7}{$k_n$-nearest neighbor universal consistency for graphs\relax }{section.B}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Simulation}{7}{section.C}}
\newlabel{simulation}{{C}{7}{Simulation\relax }{section.C}{}}
\citation{VarshneyChklovskii09}
\citation{ChalasaniBargmann07}
\citation{VaziriShank08}
\citation{HelmstaedterDenk08}
\citation{Mishchenko09}
\citation{LuLichtman09}
\citation{deBonoMaricq05}
\citation{BuckinghamSattelle08}
\bibdata{biblist}
\bibstyle{ieeetr}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces C.\nobreakspace  {}elegans graph classification simulation results. $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n)$ is plotted as a function of class-conditional training sample size $n_j$, suggesting that for $\varepsilon =0.1$ we can determine that $\@mathcal {M}\begingroup \setbox \z@ \hbox {\thinmuskip 0mu \medmuskip \m@ne mu\thickmuskip \@ne mu \setbox \tw@ \hbox {${\sim }\mathsurround \z@ $}\kern -\wd \tw@ ${}{\sim }{}\mathsurround \z@ $}\edef [{\endgroup \let \binrel@@ \relax }[\binrel@@ {\mathop {\kern \z@ {\sim }}\limits ^{\varepsilon }}_F \@mathcal {B}$ holds with $99\%$ confidence with just a few hundred training samples generated from $F_{BM}$. Each dot depicts an estimate for $L_{F}(g_n)$; standard errors are $(L_{F}(g_n)(1-L_{F}(g_n))/1000)^{1/2}$\leavevmode {\color  {black}; e}.g., $n_j = 180$ ; $k_n = 53$ ; $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle L$}\mathaccent "0362{L}^{1000}_{F}(g_n) = 0.057$; standard error less than 0.01. We reject $H_0: L_{F}(g^*) \geq 0.10$ at $\alpha =0.01$. $L_{F}(g^*) \approx 0$ for this simulation. }}{8}{figure.2}}
\newlabel{fig1}{{2}{8}{Simulation\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Acknowledgments}{8}{section.D}}
